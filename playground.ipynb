{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "\n",
    "from scripts.weather_collection import get_weather\n",
    "from scripts.data_collection import save_by_date\n",
    "from scripts.data_preprocessing import get_report_lemm, get_report_tfidf_vector\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "def isNaN(num):\n",
    "    return num != num\n",
    "\n",
    "HOLIDAY_DATASET = './data/1_holidays/holidays.csv'\n",
    "REGIONS_DATASET = './data/0_meta/regions.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vinnytsia\n",
      "200\n",
      "Lutsk\n",
      "200\n",
      "Dnipro\n",
      "200\n",
      "Donetsk\n",
      "200\n",
      "Zhytomyr\n",
      "200\n",
      "Uzhgorod\n",
      "200\n",
      "Zaporozhye\n",
      "200\n",
      "Ivano-Frankivsk\n",
      "200\n",
      "Kyiv\n",
      "200\n",
      "Kropyvnytskyi\n",
      "200\n",
      "Lviv\n",
      "500\n",
      "200\n",
      "Mykolaiv\n",
      "200\n",
      "Odesa\n",
      "200\n",
      "Poltava\n",
      "200\n",
      "Rivne\n",
      "200\n",
      "Sumy\n",
      "500\n",
      "200\n",
      "Ternopil\n",
      "200\n",
      "Kharkiv\n",
      "200\n",
      "Kherson\n",
      "200\n",
      "Khmelnytskyi\n",
      "200\n",
      "Cherkasy\n",
      "200\n",
      "Chernivtsi\n",
      "200\n",
      "Chernihiv\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "city_dict = {\n",
    "    'Lutsk': 1,\n",
    "    'Zhytomyr': 2,\n",
    "    'Rivne': 3,\n",
    "    'Poltava': 4,\n",
    "    'Ternopil': 5,\n",
    "    'Uzhgorod': 6,\n",
    "    'Donetsk': 7,\n",
    "    'Zaporozhye': 8,\n",
    "    'Sumy': 9,\n",
    "    'Cherkasy': 10,\n",
    "    'Vinnytsia': 11,\n",
    "    'Lviv': 12,\n",
    "    'Ivano-Frankivsk': 13,\n",
    "    'Kherson': 14,\n",
    "    'Kyiv': 15,\n",
    "    'Dnipro': 16,\n",
    "    'Chernivtsi': 17,\n",
    "    'Kropyvnytskyi': 18,\n",
    "    'Kharkiv': 19,\n",
    "    'Mykolaiv': 20,\n",
    "    'Khmelnytskyi': 21,\n",
    "    'Odesa': 22,\n",
    "    'Chernihiv': 23\n",
    "}\n",
    "\n",
    "df = []\n",
    "\n",
    "df_regions = pd.read_csv(REGIONS_DATASET, sep=',')\n",
    "\n",
    "for index, row in df_regions.iterrows():\n",
    "    print(row['center_city_en'])\n",
    "    try:\n",
    "        weather = get_weather(row['center_city_en'])\n",
    "        df_city = pd.DataFrame(weather[\"forecast\"])\n",
    "    except:\n",
    "        weather = get_weather(row['center_city_en'] + '(UA)')\n",
    "        df_city = pd.DataFrame(weather[\"forecast\"])\n",
    "    df_city['region_id'] = float(row['region_id'])\n",
    "    df_city['city'] = row['center_city_en']\n",
    "    df.append(df_city)\n",
    "\n",
    "df = pd.concat(df, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tmyro\\AppData\\Local\\Temp\\ipykernel_10924\\3608726545.py:10: FutureWarning: Passing method to DatetimeIndex.get_loc is deprecated and will raise in a future version. Use index.get_indexer([item], method=...) instead.\n",
      "  closest_holiday = holiday_df.index[holiday_df.index.get_loc(datetime, method='nearest')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['day_datetime', 'day_dew', 'day_humidity', 'day_moonphase',\n",
       "       'day_precip', 'day_precipcover', 'day_solarenergy',\n",
       "       'day_solarradiation', 'day_sunrise', 'day_sunset', 'day_temp',\n",
       "       'day_tempmax', 'day_tempmin', 'day_uvindex', 'hour_cloudcover',\n",
       "       'hour_conditions', 'hour_datetime', 'hour_dew', 'hour_humidity',\n",
       "       'hour_precip', 'hour_precipprob', 'hour_preciptype', 'hour_pressure',\n",
       "       'hour_severerisk', 'hour_snow', 'hour_snowdepth', 'hour_solarenergy',\n",
       "       'hour_solarradiation', 'hour_temp', 'hour_uvindex', 'hour_visibility',\n",
       "       'hour_winddir', 'hour_windgust', 'hour_windspeed', 'region_id', 'city',\n",
       "       'event_holiday_is_near', 'event_alarms_past_24',\n",
       "       'event_simultaneous_alarms', 'event_hours_from_last_alarm'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_holiday_df(): # custom made dataset with most \"important\" russian hollidays\n",
    "    holiday_df = pd.read_csv(HOLIDAY_DATASET, sep=';')\n",
    "    holiday_df['date'] = holiday_df['date'].apply(pd.to_datetime)\n",
    "    holiday_df = holiday_df.sort_values(by=['date'])\n",
    "    holiday_df = holiday_df.set_index('date')\n",
    "    return holiday_df\n",
    "\n",
    "def event_holiday_is_near(holiday_df, row):\n",
    "    datetime = parser.parse(f\"{row['day_datetime']} {row['hour_datetime']}\")\n",
    "    closest_holiday = holiday_df.index[holiday_df.index.get_loc(datetime, method='nearest')]\n",
    "    value = abs(pd.Timedelta(datetime - closest_holiday).days) <= 3\n",
    "    return 1.0 if value and not isNaN(value) else 0.0\n",
    "\n",
    "holiday_df = read_holiday_df()\n",
    "df['event_holiday_is_near'] = df.apply(lambda row: event_holiday_is_near(holiday_df, row), axis=1)\n",
    "df['event_alarms_past_24'] = 0.0\n",
    "df['event_simultaneous_alarms'] = 0.0\n",
    "df['event_hours_from_last_alarm'] = 0.0\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convet time to float\n",
    "df['day_sunset'] = df['day_sunset'].apply(lambda x: \n",
    "    (parser.parse(x) - dt.datetime.strptime(\"00:00:00\", \"%H:%M:%S\")).total_seconds()\n",
    ")\n",
    "df['day_sunrise'] = df['day_sunrise'].apply(lambda x: \n",
    "    (parser.parse(x) - dt.datetime.strptime(\"00:00:00\", \"%H:%M:%S\")).total_seconds()\n",
    ")\n",
    "df['datetime'] = df.apply(lambda row: f\"{row['day_datetime']} {row['hour_datetime']}\", axis=1)\n",
    "df['hour_datetime'] = df['hour_datetime'].apply(lambda x:\n",
    "    (parser.parse(x) - dt.datetime.strptime(\"00:00:00\", \"%H:%M:%S\")).total_seconds()//3600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(276, 41)\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical values\n",
    "df['hour_preciptype'] = df['hour_preciptype'].apply(lambda a: str(a) if a else np.nan)\n",
    "le = pickle.load(open('./model/preciptype_encoder_v1.pkl', 'rb'))\n",
    "df['hour_preciptype'] = le.transform(df['hour_preciptype']).astype(float)\n",
    "\n",
    "le = le = pickle.load(open('./model/conditions_encoder_v1.pkl', 'rb'))\n",
    "df['hour_conditions'] = le.transform(df['hour_conditions']).astype(float)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = df['city'].unique()\n",
    "city_dict = {cities[i]: i+1 for i in range(len(cities))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-23\n",
      "lennatizing\n",
      "2023-04-24\n",
      "2023-04-23\n",
      "lennatizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tmyro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:768: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'region_id_int', 'day_tempmax', 'day_tempmin', 'day_temp',\n",
       "       'day_dew', 'day_humidity', 'day_precip', 'day_precipcover',\n",
       "       'day_solarradiation',\n",
       "       ...\n",
       "       'isw_zone', 'year', 'month', 'day', 'day_of_week', 'season_winter',\n",
       "       'season_spring', 'season_summer', 'season_fall', 'city_id'],\n",
       "      dtype='object', length=767)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_yesterday_report(day_str):\n",
    "    date = parser.parse(day_str) - dt.timedelta(days=1)\n",
    "    while 'Error' in save_by_date(date):\n",
    "        date -= dt.timedelta(days=1)\n",
    "    lemm = get_report_lemm(f\"./Reports/{date.strftime('%Y-%m-%d')}.html\")\n",
    "    tfidf_vector = get_report_tfidf_vector(lemm)\n",
    "    return pd.concat([pd.DataFrame([day_str], columns=['date_tomorrow_datetime']), tfidf_vector], axis=1)\n",
    "\n",
    "df_tfidf = []\n",
    "\n",
    "for day in df['day_datetime'].unique():\n",
    "    df_tfidf.append(get_yesterday_report(day))\n",
    "\n",
    "df_tfidf = pd.concat(df_tfidf, axis=0, ignore_index=True)\n",
    "\n",
    "# Merge weather events dataset with yesterday report tfidf matrix (takes 2m to execute)\n",
    "df = df.merge(df_tfidf.add_prefix(\"isw_\"),\n",
    "                                how=\"left\",\n",
    "                                left_on=\"day_datetime\",\n",
    "                                right_on=\"isw_date_tomorrow_datetime\")\n",
    "\n",
    "# Fillna\n",
    "df.fillna(0.0, inplace=True)\n",
    "\n",
    "# Normalize\n",
    "df['region_id_int'] = df['region_id'].astype(int)\n",
    "scaler = pickle.load(open('model/scaler_v1.pkl', 'rb'))\n",
    "df_float_values = df[scaler.get_feature_names_out()]\n",
    "df_float_values_scaled = pd.DataFrame(scaler.transform(df_float_values), columns=df_float_values.columns)\n",
    "df = pd.concat([df[['datetime', 'region_id_int', 'day_datetime', 'city']], df_float_values_scaled], axis=1)\n",
    "\n",
    "df['day_datetime'] = pd.to_datetime(df['day_datetime'])\n",
    "df['year'] = df['day_datetime'].dt.year\n",
    "df['month'] = df['day_datetime'].dt.month\n",
    "df['day'] = df['day_datetime'].dt.day\n",
    "df['day_of_week'] = df['day_datetime'].dt.dayofweek\n",
    "\n",
    "df['season'] = (df['day_datetime'].dt.month % 12 // 3).replace({0: 'winter', 1: 'spring', 2: 'summer', 3: 'fall'})\n",
    "df_seasons = pd.get_dummies(df, columns=['season']).reindex(columns=['season_fall', 'season_spring', 'season_summer', 'season_winter'], fill_value=0)\n",
    "df = pd.concat([df.drop(['season'], axis=1), df_seasons], axis=1)\n",
    "\n",
    "df['city_id'] = df['city'].map(city_dict)\n",
    "df.drop(['city', 'day_datetime'], axis=1, inplace=True)\n",
    "\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tmyro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but AdaBoostClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pickle.load(open('./model/6__AdaBoost__v2.pkl', 'rb'))\n",
    "model.predict(df.drop(['datetime', 'region_id_int'], axis=1).values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
