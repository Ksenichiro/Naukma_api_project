{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "from num2words import num2words\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Stepan_Kalika\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment and process once\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define correction funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_one_letter_word(data):\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if len(w) > 1:\n",
    "            new_text = new_text + ' ' + w                                                                                                                                                                                                                                                                                             \n",
    "    return new_text\n",
    "\n",
    "def convert_lower_case(data):\n",
    "      return np.char.lower(data)\n",
    "\n",
    "             \n",
    "def remove_stop_words(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_stop_words = {\"no\",\"not\"}\n",
    "    stop_words = stop_words - stop_stop_words                        \n",
    "\n",
    "    words = word_tokenize(str(data))\n",
    "\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text +\" \"+ w\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+—./:;<=>7@[\\]^_'{|}~\\n\"\n",
    "\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "\n",
    "    data = np.char.replace(data, ',', \"\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def remove_map_line(data):\n",
    "    return np.char.replace(data, \"Click here to expand the map below.\", \"\")\n",
    "\n",
    "def remove_map_line_v2(data):\n",
    "    return np.char.replace(data, \"Click here to see ISW’s interactive map of the Russian invasion of Ukraine. This map is updated daily alongside the static maps present in this report.\", \"\")\n",
    "\n",
    "def convert_numbers(data):\n",
    "\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \" \"\n",
    "    for w in tokens:\n",
    "        if w.isdigit():\n",
    "            if int(w)<1000000000000:\n",
    "                w = num2words (w)\n",
    "            else:\n",
    "                w = ''\n",
    "        new_text = new_text +\" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "                                                \n",
    "    return new_text\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "\n",
    "    tokens = word_tokenize(str(data))\n",
    "\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "\n",
    "def lemmatizing(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + lemmatizer.lemmatize(w)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, word_root_algo=\"lemm\"):\n",
    "    data = remove_map_line(data)\n",
    "    data = remove_map_line_v2(data)\n",
    "    data = remove_one_letter_word(data)\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe (data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers (data)\n",
    "\n",
    "    if word_root_algo == \"lemm\":\n",
    "        print (\"lennatizing\")\n",
    "        data = lemmatizing(data) #needed again as we need to lemmatize the words\n",
    "    else:\n",
    "        print(\"stemming\")\n",
    "        data = stemming(data) #needed again as we need to stem the words\n",
    "\n",
    "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_words(text):\n",
    "    \"\"\"\n",
    "    Creates a bag of words (a dictionary of word frequencies) from a string.\n",
    "    \"\"\"\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Initialize an empty dictionary\n",
    "    bag_of_words = {}\n",
    "\n",
    "    # Loop over each word and count its frequency\n",
    "    for word in words:\n",
    "        if word in bag_of_words:\n",
    "            bag_of_words[word] += 1\n",
    "        else:\n",
    "            bag_of_words[word] = 1\n",
    "\n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_term_frequency(bag_of_words):\n",
    "   \n",
    "    # Calculate the total number of words in the bag\n",
    "    total_words = sum(bag_of_words.values())\n",
    "\n",
    "    # Initialize an empty dictionary\n",
    "    term_frequency = {}\n",
    "\n",
    "    # Loop over each word in the bag and calculate its term frequency\n",
    "    for word, frequency in bag_of_words.items():\n",
    "        term_frequency[word] = frequency / total_words\n",
    "\n",
    "    return term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = \"Reports\"\n",
    "OUTPUT_FOLDER = \"data_csv\"\n",
    "OUTPUT_FILE = \"all_days.csv\"\n",
    "import glob\n",
    "files_by_days = glob.glob(f\"{INPUT_FOLDER}/*.html\")\n",
    "#files_by_days = glob.glob(\"01-02-2023.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>lemm</th>\n",
       "      <th>stemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>html</td>\n",
       "      <td>SNAZ units throughout Kherson City on March 1,...</td>\n",
       "      <td>snaz unit throughout kherson citi march ukrai...</td>\n",
       "      <td>snaz unit throughout kherson citi march ukrai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>html</td>\n",
       "      <td>\\n\\nRussian forces did not make any major adv...</td>\n",
       "      <td>russian forc not make major advanc march twen...</td>\n",
       "      <td>russian forc not make major advanc march twen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>html</td>\n",
       "      <td>\\nRussian forces made minor advances in the on...</td>\n",
       "      <td>russian forc made minor advanc ongo offens ea...</td>\n",
       "      <td>russian forc made minor advanc ongo offen eas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>html</td>\n",
       "      <td>\\nRussian forces are focusing on digging in an...</td>\n",
       "      <td>russian forc focus dig reinforc defens posit ...</td>\n",
       "      <td>russian forc focu dig reinforc defen posit kh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>html</td>\n",
       "      <td>\\nClick here to see ISW's interactive map of t...</td>\n",
       "      <td>click see isw interact map russian invas ukra...</td>\n",
       "      <td>click see isw interact map russian inva ukrai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date                                               text  \\\n",
       "0    html  SNAZ units throughout Kherson City on March 1,...   \n",
       "178  html   \\n\\nRussian forces did not make any major adv...   \n",
       "179  html  \\nRussian forces made minor advances in the on...   \n",
       "180  html  \\nRussian forces are focusing on digging in an...   \n",
       "181  html  \\nClick here to see ISW's interactive map of t...   \n",
       "\n",
       "                                                  lemm  \\\n",
       "0     snaz unit throughout kherson citi march ukrai...   \n",
       "178   russian forc not make major advanc march twen...   \n",
       "179   russian forc made minor advanc ongo offens ea...   \n",
       "180   russian forc focus dig reinforc defens posit ...   \n",
       "181   click see isw interact map russian invas ukra...   \n",
       "\n",
       "                                                 stemm  \n",
       "0     snaz unit throughout kherson citi march ukrai...  \n",
       "178   russian forc not make major advanc march twen...  \n",
       "179   russian forc made minor advanc ongo offen eas...  \n",
       "180   russian forc focu dig reinforc defen posit kh...  \n",
       "181   click see isw interact map russian inva ukrai...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for file in files_by_days:\n",
    "    name = file.split(\".\")[1]\n",
    "    name = name.replace(\"\\\\\", \"\") \n",
    "    #print(name)  # \n",
    "    d = {} \n",
    "# Open the HTML file\n",
    "    with open(file, encoding=\"utf8\") as file:\n",
    "            \n",
    "\n",
    "            soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "            # Extract the text from the HTML\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # Find the index of the first occurrence of \"ET\"\n",
    "            index = text.find(\"ET\")\n",
    "\n",
    "            # Extract the text after the first occurrence of \"ET\"\n",
    "            text = text[2+index:]\n",
    "            index = text.rfind(\"[1]\")\n",
    "            text =text[:index]\n",
    "            text = re.sub(r'\\[.*?\\]', '', text)\n",
    "            # Print the text\n",
    "            #print(text)\n",
    "            lemm = preprocess(text)\n",
    "            stemm = preprocess(text, \"stemm\")\n",
    "            \n",
    "            #print(text)\n",
    "            #sample = create_bag_of_words(text)\n",
    "            #sample_fr = calculate_term_frequency(sample)\n",
    "            \n",
    "            d = {\n",
    "                \"date\":name,\n",
    "                \"text\":text,\n",
    "                \"lemm\":lemm,\n",
    "                \"stemm\":stemm\n",
    "            }\n",
    "            all_data.append(d)\n",
    "\n",
    "df = pd.DataFrame.from_dict(all_data)\n",
    "df = df.sort_values(by = ['date'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['lemm'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "282"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(282, 5745)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(max_df = 0.98, min_df = 2)\n",
    "word_count_vector = cv.fit_transform(docs)\n",
    "\n",
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<282x5745 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 154221 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model folder needed to be created previously !!!\n",
    "with open(\"model/count_vectorizer_v1.pkl\", 'wb') as handle:\n",
    "    pickle.dump(cv, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF_IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfTransformer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf = True, use_idf = True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model/tfidf_transformer_v1.pkl\", 'wb') as handle:\n",
    "    pickle.dump(tfidf_transformer, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>advanc</th>\n",
       "      <td>1.021429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>luhansk</th>\n",
       "      <td>1.021429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area</th>\n",
       "      <td>1.025046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>staff</th>\n",
       "      <td>1.028676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>combat</th>\n",
       "      <td>1.028676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>envoy</th>\n",
       "      <td>5.546835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t0803</th>\n",
       "      <td>5.546835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>envis</th>\n",
       "      <td>5.546835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nexu</th>\n",
       "      <td>5.546835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nazism</th>\n",
       "      <td>5.546835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5745 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idf_weights\n",
       "advanc      1.021429\n",
       "luhansk     1.021429\n",
       "area        1.025046\n",
       "staff       1.028676\n",
       "combat      1.028676\n",
       "...              ...\n",
       "envoy       5.546835\n",
       "t0803       5.546835\n",
       "envis       5.546835\n",
       "nexu        5.546835\n",
       "nazism      5.546835\n",
       "\n",
       "[5745 rows x 1 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names_out(), columns=[\"idf_weights\"])\n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector = tfidf_transformer.transform(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<282x5745 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 154221 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
