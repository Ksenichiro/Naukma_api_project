{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aabe0f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "from num2words import num2words\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a85f2395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and process once\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c00336",
   "metadata": {},
   "source": [
    "# Define correction funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "961af85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_one_letter_word(data):\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if len(w) > 1:\n",
    "            new_text = new_text + ' ' + w                                                                                                                                                                                                                                                                                             \n",
    "    return new_text\n",
    "\n",
    "def convert_lower_case(data):\n",
    "      return np.char.lower(data)\n",
    "\n",
    "             \n",
    "def remove_stop_words(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_stop_words = {\"no\",\"not\"}\n",
    "    stop_words = stop_words - stop_stop_words                        \n",
    "\n",
    "    words = word_tokenize(str(data))\n",
    "\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text +\" \"+ w\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+—./:;<=>7@[\\]^_'{|}~\\n\"\n",
    "\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "\n",
    "    data = np.char.replace(data, ',', \"\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def remove_map_line(data):\n",
    "    return np.char.replace(data, \"Click here to expand the map below.\", \"\")\n",
    "\n",
    "def remove_map_line_v2(data):\n",
    "    return np.char.replace(data, \"Click here to see ISW’s interactive map of the Russian invasion of Ukraine. This map is updated daily alongside the static maps present in this report.\", \"\")\n",
    "\n",
    "def convert_numbers(data):\n",
    "\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \" \"\n",
    "    for w in tokens:\n",
    "        if w.isdigit():\n",
    "            if int(w)<1000000000000:\n",
    "                w = num2words (w)\n",
    "            else:\n",
    "                w = ''\n",
    "        new_text = new_text +\" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "                                                \n",
    "    return new_text\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "\n",
    "    tokens = word_tokenize(str(data))\n",
    "\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "\n",
    "def lemmatizing(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + lemmatizer.lemmatize(w)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8ddee9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, word_root_algo=\"lemm\"):\n",
    "    data = remove_map_line(data)\n",
    "    data = remove_map_line_v2(data)\n",
    "    data = remove_one_letter_word(data)\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe (data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers (data)\n",
    "\n",
    "                                                   \n",
    "\n",
    "    if word_root_algo == \"lemm\":\n",
    "#        print (\"lennatizing\")\n",
    "        data = lemmatizing(data) #needed again as we need to lemmatize the words\n",
    "\n",
    "                  \n",
    "\n",
    "                       \n",
    "\n",
    "    else:\n",
    "#        print(\"stemming\")\n",
    "        data = stemming(data) #needed again as we need to stem the words\n",
    "\n",
    "         \n",
    "\n",
    "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e176573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_words(text):\n",
    "    \"\"\"\n",
    "    Creates a bag of words (a dictionary of word frequencies) from a string.\n",
    "    \"\"\"\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Initialize an empty dictionary\n",
    "    bag_of_words = {}\n",
    "\n",
    "    # Loop over each word and count its frequency\n",
    "    for word in words:\n",
    "        if word in bag_of_words:\n",
    "            bag_of_words[word] += 1\n",
    "        else:\n",
    "            bag_of_words[word] = 1\n",
    "\n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9dc7dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_term_frequency(bag_of_words):\n",
    "   \n",
    "    # Calculate the total number of words in the bag\n",
    "    total_words = sum(bag_of_words.values())\n",
    "\n",
    "    # Initialize an empty dictionary\n",
    "    term_frequency = {}\n",
    "\n",
    "    # Loop over each word in the bag and calculate its term frequency\n",
    "    for word, frequency in bag_of_words.items():\n",
    "        term_frequency[word] = frequency / total_words\n",
    "\n",
    "    return term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "67ee5952",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = \".\"\n",
    "OUTPUT_FOLDER = \"data_csv\"\n",
    "OUTPUT_FILE = \"all_days.csv\"\n",
    "import glob\n",
    "files_by_days = glob.glob(f\"{INPUT_FOLDER}/*.html\")\n",
    "#files_by_days = glob.glob(\"01-02-2023.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5bf6f5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>lemm</th>\n",
       "      <th>stemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>html</td>\n",
       "      <td>\\nClick here to see ISW’s interactive map of ...</td>\n",
       "      <td>ukrainian offici continu warn russia intent c...</td>\n",
       "      <td>ukrainian offici continu warn russia intent c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date                                               text  \\\n",
       "0  html   \\nClick here to see ISW’s interactive map of ...   \n",
       "\n",
       "                                                lemm  \\\n",
       "0   ukrainian offici continu warn russia intent c...   \n",
       "\n",
       "                                               stemm  \n",
       "0   ukrainian offici continu warn russia intent c...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for file in files_by_days:\n",
    "    name = file.split(\".\")[1]\n",
    "    name = name.replace(\"\\\\\", \"\") \n",
    "    #print(name)  # \n",
    "    d = {} \n",
    "# Open the HTML file\n",
    "    with open(file, encoding=\"utf8\") as file:\n",
    "            \n",
    "\n",
    "            soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "            # Extract the text from the HTML\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # Find the index of the first occurrence of \"ET\"\n",
    "            index = text.find(\"ET\")\n",
    "\n",
    "            # Extract the text after the first occurrence of \"ET\"\n",
    "            text = text[2+index:]\n",
    "            index = text.rfind(\"[1]\")\n",
    "            text =text[:index]\n",
    "            text = re.sub(r'\\[.*?\\]', '', text)\n",
    "            # Print the text\n",
    "            #print(text)\n",
    "            lemm = preprocess(text)\n",
    "            stemm = preprocess(text, \"stemm\")\n",
    "            \n",
    "            #print(text)\n",
    "            #sample = create_bag_of_words(text)\n",
    "            #sample_fr = calculate_term_frequency(sample)\n",
    "            \n",
    "            d = {\n",
    "                \"date\":name,\n",
    "                \"text\":text,\n",
    "                \"lemm\":lemm,\n",
    "                \"stemm\":stemm\n",
    "            }\n",
    "            all_data.append(d)\n",
    "\n",
    "df = pd.DataFrame.from_dict(all_data)\n",
    "df = df.sort_values(by = ['date'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45591bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['lemm'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b8cbac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e4b8833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(370, 6950)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(max_df = 0.98, min_df = 2)\n",
    "word_count_vector = cv.fit_transform(docs)\n",
    "\n",
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "afdb830f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<370x6950 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 227768 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f2147b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model folder needed to be created previously !!!\n",
    "with open(\"model/count_vectorizer_v1.pkl\", 'wb') as handle:\n",
    "    pickle.dump(cv, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a829b361",
   "metadata": {},
   "source": [
    "# TF_IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "de76f793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf = True, use_idf = True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "782cd830",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model/tfidf_transformer_v1.pkl\", 'wb') as handle:\n",
    "    pickle.dump(tfidf_transformer, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0d505001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kharkiv</th>\n",
       "      <td>1.021799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>luhansk</th>\n",
       "      <td>1.021799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main</th>\n",
       "      <td>1.024558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>staff</th>\n",
       "      <td>1.024558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>posit</th>\n",
       "      <td>1.027324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stood</th>\n",
       "      <td>5.817590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stonewal</th>\n",
       "      <td>5.817590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>op</th>\n",
       "      <td>5.817590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stohnii</th>\n",
       "      <td>5.817590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zyuganov</th>\n",
       "      <td>5.817590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6950 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          idf_weights\n",
       "kharkiv      1.021799\n",
       "luhansk      1.021799\n",
       "main         1.024558\n",
       "staff        1.024558\n",
       "posit        1.027324\n",
       "...               ...\n",
       "stood        5.817590\n",
       "stonewal     5.817590\n",
       "op           5.817590\n",
       "stohnii      5.817590\n",
       "zyuganov     5.817590\n",
       "\n",
       "[6950 rows x 1 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names_out(), columns=[\"idf_weights\"])\n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1998968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector = tfidf_transformer.transform(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d907d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
